# -*- mode: ruby -*-
# vi: set ft=ruby :

# Vagrantfile API/syntax version. Don't touch unless you know what you're doing!
VAGRANTFILE_API_VERSION = "2"

FABRIC8_VERSION = ENV['FABRIC8_VERSION'] || "2.2.1"
if !FABRIC8_VERSION || FABRIC8_VERSION.end_with?("SNAPSHOT")
  MVN_URL='https://oss.sonatype.org/service/local/artifact/maven/redirect?r=snapshots&e=json&c=kubernetes&g=io.fabric8.apps&a=${app}&v=' + FABRIC8_VERSION
else
  MVN_URL="http://central.maven.org/maven2/io/fabric8/apps/${app}/#{FABRIC8_VERSION}/${app}-#{FABRIC8_VERSION}-kubernetes.json"
end


Vagrant.require_version ">= 1.7.2"

$provisionScript = <<SCRIPT
if [ -d '/var/lib/openshift' ]; then
  exit 0
fi

mkdir /tmp/openshift
echo "Downloading OpenShift binaries..."
curl -sSL https://github.com/openshift/origin/releases/download/v1.0.0/openshift-origin-v1.0.0-67617dd-linux-amd64.tar.gz | tar xzv -C /tmp/openshift
mv /tmp/openshift/* /usr/bin/

mkdir /var/lib/openshift
restorecon -v /var/lib/openshift

cat <<EOF > /usr/lib/systemd/system/openshift.service
[Unit]
Description=OpenShift
Requires=docker.service network.service
After=network.service
[Service]
ExecStart=/usr/bin/openshift start --master=172.28.128.4 --cors-allowed-origins=.*
WorkingDirectory=/var/lib/openshift/
[Install]
WantedBy=multi-user.target
EOF

systemctl daemon-reload
systemctl enable openshift.service
systemctl start openshift.service

mkdir -p ~/.kube/
ln -s /var/lib/openshift/openshift.local.config/master/admin.kubeconfig ~/.kube/config

while true; do
  curl -k -s -f -o /dev/null --connect-timeout 1 https://localhost:8443/healthz/ready && break || sleep 1
done

oadm policy add-cluster-role-to-user cluster-admin admin
oadm router --create --credentials=/var/lib/openshift/openshift.local.config/master/openshift-router.kubeconfig
oadm registry --create --credentials=/var/lib/openshift/openshift.local.config/master/openshift-registry.kubeconfig
oadm new-project fabric8 --description="fabric8 Apps"

oc delete scc restricted
cat <<EOF | oc create -f -
---
  apiVersion: v1
  groups:
  - system:authenticated
  kind: SecurityContextConstraints
  metadata:
    name: restricted
  runAsUser:
    type: RunAsAny
  seLinuxContext:
    type: MustRunAs
EOF

oc delete scc privileged
cat <<EOF | oc create -f -
---
  allowHostDirVolumePlugin: true
  allowPrivilegedContainer: true
  apiVersion: v1
  groups:
  - system:cluster-admins
  - system:nodes
  kind: SecurityContextConstraints
  metadata:
    name: privileged
  runAsUser:
    type: RunAsAny
  seLinuxContext:
    type: RunAsAny
  users:
  - system:serviceaccount:openshift-infra:build-controller
  - system:serviceaccount:default:default
  - system:serviceaccount:default:fabric8
EOF

cat <<EOF | oc create -f -
---
  apiVersion: "v1"
  kind: "Secret"
  metadata:
    name: "openshift-cert-secrets"
  data:
    root-cert: "$(base64 -w 0 /var/lib/openshift/openshift.local.config/master/ca.crt)"
    admin-cert: "$(base64 -w 0 /var/lib/openshift/openshift.local.config/master/admin.crt)"
    admin-key: "$(base64 -w 0 /var/lib/openshift/openshift.local.config/master/admin.key)"
EOF

cat <<EOF | oc create -f -
---
  apiVersion: v1
  kind: ServiceAccount
  metadata:
    name: fabric8
  secrets:
    -
      name: openshift-cert-secrets
EOF

# Create route to registry. Please not that this route can be used for applications to find
# the images. Since it goes through the router this is not optimal for production setups
# when internally images are fetched. 
cat <<EOF | oc create -f -
{
    "kind": "Route",
    "apiVersion": "v1",
    "metadata": {
        "name": "docker-registry-route"
    },
    "spec": {
        "host": "docker-registry.vagrant.local",
        "to": {
            "kind": "Service",
            "name": "docker-registry"
        }
    }
}
EOF
echo "127.0.0.1    docker-registry.vagrant.local" >> /etc/hosts

echo "Installing templates from fabric8 version #{FABRIC8_VERSION}"
for app in base management logging metrics cdelivery-core cdelivery ipaas kitchen-sink; do
  echo "Installing ${app} template"
  echo "About to install template: #{MVN_URL}"
  oc create -f #{MVN_URL}

  # for snapshots:
  #curl –silent -o /tmp/${app}.json  -L -H "Accept: */*" "#{MVN_URL}"
  #oc create -f /tmp/${app}.json
done

# now lets instantiate the console
oc process base | oc create -f -

# now lets create a route for it:
cat <<EOF | oc create -f -
{
    "kind": "Route",
    "apiVersion": "v1",
    "metadata": {
        "name": "fabric8"
    },
    "spec": {
        "host": "fabric8.vagrant.local",
        "to": {
            "kind": "Service",
            "name": "fabric8"
        }
    }
}
EOF

echo
echo
echo
echo
echo The OpenShift console is at: https://172.28.128.4:8443/console
echo
echo Now we need to wait for the 'fabric8' pod to startup.
echo This will take a few minutes as it downloads some docker images.
echo
echo Please be patient!
echo --------------------------------------------------------------
echo
echo Now might be a good time to setup your host machine to work with OpenShift
echo
echo "* Download a recent release of the binaries and add them to your PATH:
echo
echo "    https://github.com/openshift/origin/releases/"
echo
echo "* Set the following environment variables:
echo
echo "    export KUBERNETES_MASTER=https://172.28.128.4:8443"
echo "    export KUBERNETES_DOMAIN=vagrant.local"
echo "    export KUBERNETES_TRUST_CERT=true"
echo "    export DOCKER_HOST=tcp://vagrant.local:2375"
echo
echo
echo Now login to OpenShift via this command:
echo
echo "    oc login --insecure-skip-tls-verify=false https://172.28.128.4:8443"
echo
echo Then enter admin/admin for user/password.
echo
echo Over time your token may expire and you will need to reauthenticate via:
echo
echo "    oc login"
echo
echo
echo Now to see the status of the system:
echo
echo "    oc get pods"
echo
echo or you can watch from the command line via one of these commands:
echo
echo "    watch oc get pods"
echo "    oc get pods --watch"
echo
echo
echo --------------------------------------------------------------
echo
echo Now waiting for the fabric8 pod to start Running....

until oc get pods -l component=console,provider=fabric8  | grep -m 1 "Running"; do sleep 1 ; done

echo
echo --------------------------------------------------------------
echo Fabric8 pod is running! Who-hoo!
echo --------------------------------------------------------------
echo
echo
echo Now open the fabric8 console at:
echo
echo "    http://fabric8.vagrant.local/"
echo
echo When you first open your browser Chrome will say:
echo
echo "   Your connection is not private"
echo
echo "* Don't panic!"
echo
echo "* Click on the small 'Advanced' link on the bottom left"
echo
echo "* Now click on the link that says 'Proceed to fabric8.vagrant.local (unsafe)' bottom left"
echo
echo "* Now the browser should redirect to the login page. Enter admin/admin"
echo
echo "* You should now be in the main fabric8 console. That was easy eh! :)"
echo
echo "* Make sure you start off in the 'default' namespace."
echo
echo
echo To install more applications click the Run... button on the Apps tab.
echo
echo We love feedback: http://fabric8.io/community/
echo Have fun!

SCRIPT

unless Vagrant.has_plugin?("vagrant-hostmanager")
  raise 'Please type this command then try again: vagrant plugin install vagrant-hostmanager'
end

Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|
  config.hostmanager.enabled = true
  config.hostmanager.manage_host = true
  config.hostmanager.ignore_private_ip = false
  config.hostmanager.include_offline = true

  config.hostmanager.aliases = %w(vagrant.local fabric8.vagrant.local fabric8-master.vagrant.local jenkins.vagrant.local gogs.vagrant.local nexus.vagrant.local hubot-web-hook.vagrant.local letschat.vagrant.local kibana.vagrant.local taiga.vagrant.local fabric8-forge.vagrant.local quickstart-camelservlet.vagrant.local quickstart-cxf-cdi.vagrant.local)


  config.vm.box = "jimmidyson/centos-7.1"
  config.vm.box_version = "= 1.1.0"

  config.vm.network "private_network", ip: "172.28.128.4"

  config.vm.hostname = "fabric8-master.vagrant.local"


  config.vm.provider "virtualbox" do |v|
    v.memory = 4096
    v.cpus = 2
    v.customize ["modifyvm", :id, "--natdnshostresolver1", "on"]
  end


  config.vm.provision "shell", inline: $provisionScript

end
