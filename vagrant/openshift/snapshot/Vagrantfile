# coding: utf-8
# -*- mode: ruby -*-
# vi: set ft=ruby :

# Vagrantfile API/syntax version. Don't touch unless you know what you're doing!
VAGRANTFILE_API_VERSION = "2"

FABRIC8_VERSION=ENV['FABRIC8_VERSION'] || "2.2.14"
PREFETCH_IMAGES=ENV['PREFETCH_IMAGES'] || "1"

if !FABRIC8_VERSION || FABRIC8_VERSION.end_with?("SNAPSHOT")
  MVN_URL='https://oss.sonatype.org/service/local/artifact/maven/redirect?r=snapshots&e=json&c=kubernetes&g=io.fabric8.apps&a=${app}&v=' + FABRIC8_VERSION
  MVN_QUICKSTART_URL='https://oss.sonatype.org/service/local/artifact/maven/redirect?r=snapshots&e=json&c=kubernetes&g=io.fabric8.jube.images.fabric8&a=${app}&v=' + FABRIC8_VERSION
else
  MVN_URL="http://central.maven.org/maven2/io/fabric8/apps/${app}/#{FABRIC8_VERSION}/${app}-#{FABRIC8_VERSION}-kubernetes.json"
  MVN_QUICKSTART_URL="http://central.maven.org/maven2/io/fabric8/jube/images/fabric8/${app}/#{FABRIC8_VERSION}/${app}-#{FABRIC8_VERSION}-kubernetes.json"
end


Vagrant.require_version ">= 1.7.2"

$provisionScript = <<SCRIPT
if [ -d '/var/lib/openshift' ]; then
  exit 0
fi

mkdir /tmp/openshift
echo "Downloading OpenShift binaries..."
curl --retry 999 --retry-max-time 0  -sSL https://github.com/openshift/origin/releases/download/v1.0.3/openshift-origin-v1.0.3-1695461-linux-amd64.tar.gz | tar xzv -C /tmp/openshift
mv /tmp/openshift/* /usr/bin/

mkdir -p /var/lib/openshift/openshift.local.manifests

pushd /var/lib/openshift
/usr/bin/openshift start --master=172.28.128.4 --cors-allowed-origins=.* --hostname=172.28.128.4 --write-config=/var/lib/openshift/openshift.local.config
cat <<EOF >> /var/lib/openshift/openshift.local.config/node-172.28.128.4/node-config.yaml
kubeletArguments:
  "read-only-port":
    - "10255"
EOF
sed -i 's|^podManifestConfig: null|podManifestConfig:\\n  path: /var/lib/openshift/openshift.local.manifests\\n  fileCheckIntervalSeconds: 10|' /var/lib/openshift/openshift.local.config/node-172.28.128.4/node-config.yaml
popd
restorecon -Rv /var/lib/openshift

cat <<EOF > /usr/lib/systemd/system/openshift.service
[Unit]
Description=OpenShift
Requires=docker.service network.service
After=network.service
[Service]
ExecStart=/usr/bin/openshift start --master-config=/var/lib/openshift/openshift.local.config/master/master-config.yaml --node-config=/var/lib/openshift/openshift.local.config/node-172.28.128.4/node-config.yaml
WorkingDirectory=/var/lib/openshift/
[Install]
WantedBy=multi-user.target
EOF

systemctl daemon-reload
systemctl enable openshift.service
systemctl start openshift.service

mkdir -p ~/.kube/
ln -s /var/lib/openshift/openshift.local.config/master/admin.kubeconfig ~/.kube/config

while true; do
  curl -k -s -f -o /dev/null --connect-timeout 1 https://localhost:8443/healthz/ready && break || sleep 1
done

oadm policy add-cluster-role-to-user cluster-admin admin

function downloadImages() {
  echo ""
  if [ -n "#{PREFETCH_IMAGES}" -a \\( "#{PREFETCH_IMAGES}" = 1 -o "#{PREFETCH_IMAGES}" = "true" \\) ]; then
    echo "Finding the docker images inside $1 with name $2"
    oc get $1 -ojson $2 | grep -Po '"image": ".*",' | grep -Po '(?<=image": ").*(?=",)' | xargs -n1 -ITOKEN docker pull TOKEN
  else
    echo "Skip prefetching images"
  fi
}


oc delete scc restricted
cat <<EOF | oc create -f -
---
  apiVersion: v1
  groups:
  - system:authenticated
  kind: SecurityContextConstraints
  metadata:
    name: restricted
  runAsUser:
    type: RunAsAny
  seLinuxContext:
    type: MustRunAs
EOF

oc delete scc privileged
cat <<EOF | oc create -f -
---
  allowHostDirVolumePlugin: true
  allowPrivilegedContainer: true
  apiVersion: v1
  groups:
  - system:cluster-admins
  - system:nodes
  kind: SecurityContextConstraints
  metadata:
    name: privileged
  runAsUser:
    type: RunAsAny
  seLinuxContext:
    type: RunAsAny
  users:
  - system:serviceaccount:openshift-infra:build-controller
  - system:serviceaccount:default:default
  - system:serviceaccount:default:fabric8
EOF

cat <<EOF | oc create -f -
---
  apiVersion: "v1"
  kind: "Secret"
  metadata:
    name: "openshift-cert-secrets"
  data:
    root-cert: "$(base64 -w 0 /var/lib/openshift/openshift.local.config/master/ca.crt)"
    admin-cert: "$(base64 -w 0 /var/lib/openshift/openshift.local.config/master/admin.crt)"
    admin-key: "$(base64 -w 0 /var/lib/openshift/openshift.local.config/master/admin.key)"
EOF

cat <<EOF | oc create -f -
---
  apiVersion: v1
  kind: ServiceAccount
  metadata:
    name: fabric8
  secrets:
    -
      name: openshift-cert-secrets
EOF

cat <<EOF | oc create -f -
---
  apiVersion: v1
  kind: ServiceAccount
  metadata:
    name: metrics
EOF

oadm policy add-cluster-role-to-user cluster-reader system:serviceaccount:default:metrics
oadm policy add-cluster-role-to-user cluster-admin system:serviceaccount:default:fabric8

echo "127.0.0.1    docker-registry.vagrant.f8" >> /etc/hosts

echo "Installing templates from fabric8 version #{FABRIC8_VERSION}"
for app in base management logging metrics chat cdelivery-core cdelivery apiman messaging ipaas kitchen-sink quickstarts; do
  echo "Installing ${app} template"
  echo "About to install template: #{MVN_URL}"
  oc create -f #{MVN_URL}

  # for snapshots:
  #curl –silent -o /tmp/${app}.json  -L -H "Accept: */*" "#{MVN_URL}"
  #oc create -f /tmp/${app}.json
done

echo "Installing quickstart templates from fabric8 version #{FABRIC8_VERSION}"
for app in chaos-monkey fabric8mq-producer fabric8mq-consumer; do
  echo "Installing ${app} app template"
  echo "About to install app template: #{MVN_QUICKSTART_URL}"
  oc create -f #{MVN_QUICKSTART_URL}

  # for snapshots:
  #curl –silent -o /tmp/${app}.json  -L -H "Accept: */*" "#{MVN_QUICKSTART_URL}"
  #oc create -f /tmp/${app}.json
done

# can't use the snazzy script so lets pull them by hand
#downloadImages "deploymentConfig" "router"
docker pull openshift/origin-haproxy-router:v1.0.0

oadm router --create --credentials=/var/lib/openshift/openshift.local.config/master/openshift-router.kubeconfig
oadm new-project fabric8 --description="fabric8 Apps"


# now lets create a route for kibana and console:
echo "Adding routes for the fabric8 console and kibana"
cat <<EOF | oc create -f -
{
  "kind": "List",
  "apiVersion": "v1",
  "items": [{
    "kind": "Route",
    "apiVersion": "v1",
    "metadata": {
      "name": "kibana"
    },
    "spec": {
      "host": "kibana.vagrant.f8",
      "to": {
        "kind": "Service",
        "name": "kibana"
      }
    }
  }, {
    "kind": "Route",
    "apiVersion": "v1",
    "metadata": {
      "name": "fabric8"
    },
    "spec": {
      "host": "fabric8.vagrant.f8",
      "to": {
        "kind": "Service",
        "name": "fabric8"
      }
    }
  }]
}
EOF

# Create route to registry. Please not that this route can be used for applications to find
# the images. Since it goes through the router this is not optimal for production setups
# when internally images are fetched.
cat <<EOF | oc create -f -
{
    "kind": "Route",
    "apiVersion": "v1",
    "metadata": {
        "name": "docker-registry-route"
    },
    "spec": {
        "host": "docker-registry.vagrant.f8",
        "to": {
            "kind": "Service",
            "name": "docker-registry"
        }
    }
}
EOF


cat <<EOT





The OpenShift console is at: https://172.28.128.4:8443/console

Now we need to wait for the 'fabric8' pod to startup.
This will take a few minutes as it downloads some docker images.

Please be patient!
--------------------------------------------------------------

Now might be a good time to setup your host machine to work with OpenShift

* Download a recent release of the binaries and add them to your PATH:

   https://github.com/openshift/origin/releases/

* Set the following environment variables (example is in Unix style, use 'set' for Windows):

   export KUBERNETES_DOMAIN=vagrant.f8
   export DOCKER_HOST=tcp://vagrant.f8:2375

Now login to OpenShift via this command:

   oc login https://172.28.128.4:8443

Then enter admin/admin for user/password.

Over time your token may expire and you will need to reauthenticate via:

   oc login

Now to see the status of the system:

   oc get pods

or you can watch from the command line via one of these commands:

   watch oc get pods
   oc get pods --watch

--------------------------------------------------------------
Now waiting for the fabric8 pod to download and start Running....
--------------------------------------------------------------

Downloading docker images...
EOT

downloadImages "template" "logging"
oc process logging | oc create -f -


until oc get pods -l component=console,provider=fabric8  | grep -m 1 "Running"; do sleep 1 ; done

cat <<EOT




--------------------------------------------------------------
Fabric8 pod is running! Who-hoo!
--------------------------------------------------------------

Now open the fabric8 console at:

    http://fabric8.vagrant.f8/

When you first open your browser Chrome will say:

   Your connection is not private

* Don't panic!
* Click on the small 'Advanced' link on the bottom left
* Now click on the link that says 'Proceed to fabric8.vagrant.f8 (unsafe)' bottom left
* Now the browser should redirect to the login page. Enter admin/admin
* You should now be in the main fabric8 console. That was easy eh! :)
* Make sure you start off in the 'default' namespace.

To install more applications click the Run... button on the Apps tab.

We love feedback: http://fabric8.io/community/
Have fun!

Now open the fabric8 console at:

    http://fabric8.vagrant.f8/

--------------------------------------------------------------

EOT

# now lets create the registry
oadm registry --create --credentials=/var/lib/openshift/openshift.local.config/master/openshift-registry.kubeconfig


# And install the node-local fluentd pod
cat <<'EOF' > /var/lib/openshift/openshift.local.manifests/fluentd.yaml
apiVersion: v1
kind: Pod
metadata:
  name: fluentd-elasticsearch
spec:
  containers:
  - name: fluentd-elasticsearch
    image: fabric8/fluentd-kubernetes:1.0
    securityContext:
      privileged: true
    resources:
      limits:
        cpu: 100m
    volumeMounts:
    - name: varlog
      mountPath: /var/log
    - name: varlibdockercontainers
      mountPath: /var/lib/docker/containers
      readOnly: true
    env:
    - name: ES_HOST
      value: $(ELASTICSEARCH_SERVICE_HOST)
  volumes:
  - name: varlog
    hostPath:
      path: /var/log
  - name: varlibdockercontainers
    hostPath:
      path: /var/lib/docker/containers
EOF


SCRIPT

$windows = (/cygwin|mswin|mingw|bccwin|wince|emx/ =~ RUBY_PLATFORM) != nil

if $windows && Vagrant.has_plugin?("vagrant-hostmanager")
  raise 'Conflicting vagrant plugin detected - please uninstall & then try again: vagrant plugin uninstall vagrant-hostmanager'
end
$pluginToCheck = $windows ? "vagrant-hostmanager-fabric8" : "landrush"
unless Vagrant.has_plugin?($pluginToCheck)
  raise 'Please type this command then try again: vagrant plugin install ' + $pluginToCheck
end

Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|

  # Top level domain
  $tld = "f8"

  # Landrush is used together with wildcard dns entries to map all
  # routes to the proper services
  if $windows
    config.hostmanager.enabled = true
    config.hostmanager.manage_host = true
    config.hostmanager.ignore_private_ip = false
    config.hostmanager.include_offline = true

    config.hostmanager.aliases = %w(fabric8.vagrant.f8 jenkins.vagrant.f8 gogs.vagrant.f8 nexus.vagrant.f8 hubot-web-hook.vagrant.f8 letschat.vagrant.f8 kibana.vagrant.f8 taiga.vagrant.f8 fabric8-forge.vagrant.f8)
  else
    config.landrush.enabled = true
    config.landrush.tld = $tld
    config.landrush.host_ip_address = '172.28.128.4'
  end

  config.vm.box = "jimmidyson/centos-7.1"
  config.vm.box_version = "= 1.1.2"

  config.vm.network "private_network", ip: "172.28.128.4"

  config.vm.hostname = "vagrant." + $tld

  config.vm.provider "virtualbox" do |v|
    v.memory = 4096
    v.cpus = 2
    v.customize ["modifyvm", :id, "--natdnshostresolver1", "on"]
  end

  config.vm.provider :libvirt do |v|
    v.cpus = 2
    v.memory = 4096
  end

  config.vm.provision "shell", inline: $provisionScript, keep_color: true

end
