# coding: utf-8
# -*- mode: ruby -*-
# vi: set ft=ruby :

# Vagrantfile API/syntax version. Don't touch unless you know what you're doing!
VAGRANTFILE_API_VERSION = "2"

Vagrant.require_version ">= 1.7.2"

$provisionScript = <<SCRIPT
# download gofabric8
mkdir /tmp/gofabric8

# TODO for now lets use the latest build until we've a release...
#curl --retry 999 --retry-max-time 0  -sSL https://github.com/fabric8io/gofabric8/releases/download/v0.3/gofabric8-0.3-linux-amd64.tar.gz | tar xzv -C /tmp/gofabric8
curl --retry 999 --retry-max-time 0  -sSL https://fabric8-ci.fusesource.com/job/gofabric8/lastSuccessfulBuild/artifact/src/github.com/fabric8io/gofabric8/build/gofabric8 > /tmp/gofabric8/gofabric8
chmod +x /tmp/gofabric8/gofabric8
mv /tmp/gofabric8/* /usr/bin/


# setup openshift
if [ -d '/var/lib/openshift' ]; then
  exit 0
fi

mkdir /tmp/openshift
echo "Downloading OpenShift binaries..."
#curl --retry 999 --retry-max-time 0  -sSL https://github.com/openshift/origin/releases/download/v1.0.5/openshift-origin-v1.0.5-96963b6-linux-amd64.tar.gz | tar xzv -C /tmp/openshift
curl --retry 999 --retry-max-time 0  -sSL https://github.com/openshift/origin/releases/download/v1.0.4/openshift-origin-v1.0.4-757efd9-linux-amd64.tar.gz | tar xzv -C /tmp/openshift
mv /tmp/openshift/* /usr/bin/

mkdir -p /var/lib/openshift/openshift.local.manifests

pushd /var/lib/openshift
/usr/bin/openshift start --master=172.28.128.4 --cors-allowed-origins=.* --hostname=172.28.128.4 --write-config=/var/lib/openshift/openshift.local.config
cat <<EOF >> /var/lib/openshift/openshift.local.config/node-172.28.128.4/node-config.yaml
kubeletArguments:
  "read-only-port":
    - "10255"
EOF
sed -i 's|^podManifestConfig: null|podManifestConfig:\\n  path: /var/lib/openshift/openshift.local.manifests\\n  fileCheckIntervalSeconds: 10|' /var/lib/openshift/openshift.local.config/node-172.28.128.4/node-config.yaml
popd
restorecon -Rv /var/lib/openshift

cat <<EOF > /usr/lib/systemd/system/openshift.service
[Unit]
Description=OpenShift
Requires=docker.service network.service
After=network.service
[Service]
ExecStart=/usr/bin/openshift start --master-config=/var/lib/openshift/openshift.local.config/master/master-config.yaml --node-config=/var/lib/openshift/openshift.local.config/node-172.28.128.4/node-config.yaml
WorkingDirectory=/var/lib/openshift/
[Install]
WantedBy=multi-user.target
EOF

systemctl daemon-reload
systemctl enable openshift.service
systemctl start openshift.service

mkdir -p ~/.kube/
ln -s /var/lib/openshift/openshift.local.config/master/admin.kubeconfig ~/.kube/config

while true; do
  curl -k -s -f -o /dev/null --connect-timeout 1 https://localhost:8443/healthz/ready && break || sleep 1
done


oadm policy add-cluster-role-to-user cluster-admin admin

gofabric8 deploy -y
gofabric8 secrets -y
gofabric8 volume -y --host-path="/vagrant/fabric8-data"

cat <<EOT





The OpenShift console is at: https://172.28.128.4:8443/console

Now we need to wait for the 'fabric8' pod to startup.
This will take a few minutes as it downloads some docker images.

Please be patient!
--------------------------------------------------------------

Now might be a good time to setup your host machine to work with OpenShift

* Download a recent release of the binaries and add them to your PATH:

   https://github.com/openshift/origin/releases/

* Set the following environment variables (example is in Unix style, use 'set' for Windows):

   export KUBERNETES_DOMAIN=vagrant.f8
   export DOCKER_HOST=tcp://vagrant.f8:2375

Now login to OpenShift via this command:

   oc login https://172.28.128.4:8443

Then enter admin/admin for user/password.

Over time your token may expire and you will need to reauthenticate via:

   oc login

Now to see the status of the system:

   oc get pods

or you can watch from the command line via one of these commands:

   watch oc get pods
   oc get pods --watch

--------------------------------------------------------------
Now waiting for the fabric8 pod to download and start Running....
--------------------------------------------------------------

Downloading docker images...
EOT

until oc get pods -l component=console,provider=fabric8  | grep -m 1 "Running"; do sleep 1 ; done

cat <<EOT




--------------------------------------------------------------
Fabric8 pod is running! Who-hoo!
--------------------------------------------------------------

Now open the fabric8 console at:

    http://fabric8.vagrant.f8/

When you first open your browser Chrome will say:

   Your connection is not private

* Don't panic!
* Click on the small 'Advanced' link on the bottom left
* Now click on the link that says 'Proceed to fabric8.vagrant.f8 (unsafe)' bottom left
* Now the browser should redirect to the login page. Enter admin/admin
* You should now be in the main fabric8 console. That was easy eh! :)
* Make sure you start off in the 'default' namespace.

To install more applications click the Run... button on the Apps tab.

We love feedback: http://fabric8.io/community/
Have fun!

Now open the fabric8 console at:

    http://fabric8.vagrant.f8/

--------------------------------------------------------------

EOT

# create the router
oadm router --create --credentials=/var/lib/openshift/openshift.local.config/master/openshift-router.kubeconfig --service-account=router

# now lets create the registry
oadm registry --create --credentials=/var/lib/openshift/openshift.local.config/master/openshift-registry.kubeconfig


# And install the node-local fluentd pod
cat <<'EOF' > /var/lib/openshift/openshift.local.manifests/fluentd.yaml
apiVersion: v1
kind: Pod
metadata:
  name: fluentd-elasticsearch
spec:
  containers:
  - name: fluentd-elasticsearch
    image: fabric8/fluentd-kubernetes:1.0
    securityContext:
      privileged: true
    resources:
      limits:
        cpu: 100m
    volumeMounts:
    - name: varlog
      mountPath: /var/log
    - name: varlibdockercontainers
      mountPath: /var/lib/docker/containers
      readOnly: true
    env:
    - name: ES_HOST
      value: $(ELASTICSEARCH_SERVICE_HOST)
  volumes:
  - name: varlog
    hostPath:
      path: /var/log
  - name: varlibdockercontainers
    hostPath:
      path: /var/lib/docker/containers
EOF


SCRIPT

$windows = (/cygwin|mswin|mingw|bccwin|wince|emx/ =~ RUBY_PLATFORM) != nil

if $windows && Vagrant.has_plugin?("vagrant-hostmanager")
  raise 'Conflicting vagrant plugin detected - please uninstall & then try again: vagrant plugin uninstall vagrant-hostmanager'
end
$pluginToCheck = $windows ? "vagrant-hostmanager-fabric8" : "landrush"
unless Vagrant.has_plugin?($pluginToCheck)
  raise 'Please type this command then try again: vagrant plugin install ' + $pluginToCheck
end

Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|

  # Top level domain
  $tld = "f8"

  # Landrush is used together with wildcard dns entries to map all
  # routes to the proper services
  if $windows
    config.hostmanager.enabled = true
    config.hostmanager.manage_host = true
    config.hostmanager.ignore_private_ip = false
    config.hostmanager.include_offline = true

    config.hostmanager.aliases = %w(fabric8.vagrant.f8 jenkins.vagrant.f8 gogs.vagrant.f8 nexus.vagrant.f8 hubot-web-hook.vagrant.f8 letschat.vagrant.f8 kibana.vagrant.f8 taiga.vagrant.f8 fabric8-forge.vagrant.f8)
  else
    config.landrush.enabled = true
    config.landrush.tld = $tld
    config.landrush.host_ip_address = '172.28.128.4'
  end

  config.vm.box = "jimmidyson/centos-7.1"
  config.vm.box_version = "= 1.2.1"

  config.vm.network "private_network", ip: "172.28.128.4"

  config.vm.hostname = "vagrant." + $tld

  config.vm.provider "virtualbox" do |v|
    v.memory = 6096
    v.cpus = 2
    v.customize ["modifyvm", :id, "--natdnshostresolver1", "on"]
  end

  config.vm.provider :libvirt do |v|
    v.cpus = 2
    v.memory = 6096
  end

  if $windows
  else
    # config.vm.synced_folder ".", "/jenkins-workspace", type: "nfs"
  end

  config.vm.provision "shell", inline: $provisionScript, keep_color: true

end
